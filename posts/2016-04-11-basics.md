---
title: Basics
tags: Randomized Algorithm
---

# Model of computation

## Monte-Carlo
stop in finite time, answer may be incorrect.
*One-sided error*: make error only in one direction. E.g. when answer is 'y', output P['y']$\ge \epsilon$>0, when answer is 'no', output P['n']=1. So that when output is 'yes', answer must be yes. Run $t$ times, $P(err)=(1-\epsilon)^t$. With prob at least $1-\delta$, $O(\log {1\over \delta})$ iteration gives correct answer. *RP*: Randomized Polynomial.

*Two-sided error*: when answer is yes, $P(yes)=0.5+\epsilon$; when answer is no, $P(no)=0.5+\epsilon$. Majority vote after $t$ times. With prob at least $1-\delta$, $t=O(\log {1\over \delta})$ suffices. *BPP*: bounded error probabilistic polynomial

__proof__: $P(error)=\sum_{i=0}^{\lfloor t/2 \rfloor} \binom{t}{i} (0.5+\epsilon)^i (0.5-\epsilon)^{t-i} \le \sum_{i=0}^{\lfloor t/2 \rfloor} \binom{t}{i} (0.5+\epsilon)^{t/2} (0.5-\epsilon)^{t/2}$, omit.

## Las Vegas
always correct. running time may be unbounded. Expected running time is bounded. Las Vegas $\Rightarrow$ Monte Carlo: Run fixed time, output arbitrary if not finished. *ZPP*: Zero-error
Probabilistic Polynomial time.

Does randomization helps? P=RP or P=BPP? not known.

# Checking matrix multiplication
check $AB=C$. Naive: $O(n^3) or O(n^{2.376})$.
Randomized: $O(n^2)$. Use finite $S$. Pick $r=(r_1,\ldots, r_n)$ where $r_i\in S$ iid. If $AB\neq C$, then $P(ABr=Cr) \le {1\over |S|}$. Defered desicion: for $r_2, \ldots, r_n$, less than one $r_1\in S$ satisfies $ABr=Cr$.

__general__: check if $A(r)$ is true for all $r$. *searching for a witness* that $P(r)=0$.
__algorithm__: Pick $r$ indenpently for $t$ times. Output 1 if all $A(r_i)$ is true; else 0.
__limitation__: density of $\{r: A(r)=0\}$ is large.

# Check associativity of operator
Given $\circ$ and finite $X$, $|X|=n$. Check if $\circ$ is associative over $X$. naive $O(n^3)$.
Randomized: $O(n^2)$. Direct implementation is impractical because a family of binary operators exists such that number of non-associative triple $(i,j,k)$ is at most a constant (proof?). density is $O(1/n^3)$.

## Rajagopalan and Schulman [^1]
$R \in 2^X \equiv \mathcal{X}$. $r\in\{0,1\}^n$ is indicator vector: $r_i=1\Rightarrow i\in R$. $R\equiv \sum_i r_i\cdot i$
Define
$$
R+S = \sum_i (r_i\oplus s_i)\cdot i\\
R\circ S = \sum_{i,j\in X} (r_i s_j)(i\circ j)
$$
__algorithm__: pick $R,S,T$ iid from $\mathcal X$. If $(R\circ S)\circ T \neq R\circ (S\circ T)$, output 0. else 1.
__analysis__: If $\circ$ is not asociative, $P(\neq)\ge 1/8$.

Suppose $i^*, j^*, k^*$ is not associative. $\forall R_0,S_0,T_0$ such that $i^*\notin R_0, j^*\notin S_0, k^*\notin T_0$, define $R_1 = R_0 \cup \{i\}, S_1, T_1$. $\Phi = \{(R_a,S_b,T_c)|a,b,c=0,1\}, |\Phi|=8$.

Let $$
f(i,j,k)=(i\circ j)\circ k- i\circ (j\circ k)\\
f(R,S,T)=\sum_{i\in R, j\in S, k\in T} f(i,j,k)
$$
inclusion-exclusion priciple:
$$
f(i^*, j^*, k^*) = f(R_1, S_1, T_1) − f(R_1, S_1, T_0) − f(R_1, S_0, T_1) − f(R_0, S_1, T_1) +
f(R_1, S_0, T_0) + f(R_0, S1, T_0) + f(R_0, S_0, T_1) − f(R_0, S_0, T_0)
$$
At least one term of right side is not zero $\Rightarrow$ $P(\neq)\ge 1/8$

S. Rajagopalan and L. Schulman, Verifying Identities, Proc. IEEE FOCS 1996, pp. 612–
616.
